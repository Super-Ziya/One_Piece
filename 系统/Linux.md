###  Linux

####1、xshell

> 上传文件到服务器：`rz -y`
>
> 下载文件到本地：`sz + 文件`
>
> 获得权限：`sudo su` 回车后输入密码

#### 2、常用命令

>`ls` 列出文件夹里文件名称
>
>`ll` 查看文件夹信息，文件总数等详细信息
>
>`cd /` 回到主文件夹；`cd ..` 返回上一级目录
>
>`rm +文件` 删除文件（`-f` 强制删除）
>
>`rm -r +文件夹` 删除文件夹
>
>`mkdir +文件夹` 创建文件夹
>
>`mv 文件名 目标文件夹目录(例如/home/USER_zy)` 移动文件到目标文件夹
>
>`ps -ef|grep py` 查看当前进程
>
>`df -hl` 查看内存空间

####3、VI编辑

> 按ESC进入Command模式，输入
>
> > `：wq` 回车就是保存退出
> >
> > `w` 保存
> >
> > `wq!` 保存强制退出
> >
> > `q!` 强制退出

#### 4、Linux IPC 机制：

##### （1）管道 PIPE

> 实际是用于进程间通信的一段共享内存，创建管道的进程称为管道服务器，连接到一个管道的进程为管道客户机。一个进程在向管道写入数据后，另一进程就可以从管道的另一端将其读取出来

- 特点：

  - 半双工，需要双方通信时，要建立起两个管道
  - 只能用于父子进程或兄弟进程之间（具有亲缘关系的进程）。如 fork 或 exec 创建的新进程，使用 exec 创建新进程时，需要将管道的文件描述符作为参数传递给 exec 创建的新进程。 当父进程与 fork 创建的子进程直接通信时，发送数据的进程关闭读端，接受数据的进程关闭写端
  - 单独构成一种独立的文件系统：管道对于管道两端的进程而言是一个文件，但不属于某种文件系统，而是单独构成一种文件系统，且只存在与内存中
  - 数据的读出和写入：一个进程向管道中写的内容被管道另一端的进程读出。写入的内容每次都添加在管道缓冲区的末尾，且每次都从缓冲区的头部读出数据

- 实现机制：管道是由内核管理的一个缓冲区，一个缓冲区不需要很大，它被设计成为环形的数据结构，以便管道可以被循环利用。当管道中没有信息的话，从管道中读取的进程会等待，直到另一端的进程放入信息。当管道被放满信息的时候，尝试放入信息的进程会等待，直到另一端的进程取出信息。当两个进程都终结的时候，管道也自动消失。

- pipe 函数原型：通过使用底层的 read 和 write 调用来访问数据。 向 file_descriptor[1] 写数据，从 file_descriptor[0] 读数据。写入与读取原则是先进先出

- 读写规则

  - 当没有数据可读时

    > O_NONBLOCK disable：read 调用阻塞，即进程暂停执行，等到有数据来到为止
    >
    > O_NONBLOCK enable：read 调用返回 -1，errno 值为 EAGAIN

  - 当管道满时

    > O_NONBLOCK disable： write 调用阻塞，直到有进程读走数据
    > O_NONBLOCK enable：调用返回 -1，errno 值为 EAGAIN

  - 如果所有管道写端对应的文件描述符被关闭：read 返回 0

  - 如果所有管道读端对应的文件描述符被关闭：write 操作会产生信号 SIGPIPE

  - 当要写入的数据量不大于 PIPE_BUF（Posix.1 要求 PIPE_BUF 至少 512 字节）时，linux 将保证写入的原子性

  - 当要写入的数据量大于 PIPE_BUF 时，linux 不再保证写入的原子性

##### （2）命名管道 FIFO

> 是一种特殊类型的文件，在系统中以文件形式存在。克服了管道的弊端，允许没有亲缘关系的进程间通信

- 管道和命名管道的区别：
  - 对于命名管道 FIFO 来说，IO 操作和普通管道 IO 操作基本一样，主要区别是在命名管道中，管道可以是事先已经创建好的，如在命令行下执行 `mkfifo myfifo` 创建一个命名通道，必须用 open 函数显示建立连接到管道的通道，而在管道中，管道已经在主进程里创建好了，在 fork 时直接复制相关数据或用 exec 创建的新进程时把管道的文件描述符当参数传递进去
  - 一般来说 FIFO 和 PIPE 一样总处于阻塞状态。如果命名管道 FIFO 打开时设置了读权限，则读进程将一直阻塞，直到其他进程打开该 FIFO 并向管道写入数据。这个阻塞动作反过来成立。在 open 时使用 O_NONBLOCK 标志可以关闭默认的阻塞操作

##### （3）消息队列

> 消息队列是内核地址空间中的内部链表，通过 linux 内核在各个进程直接传递内容，消息顺序地发送到消息队列中，并以几种不同的方式从队列中获得，每个消息队列可以用 IPC 标识符唯一地进行识别。内核中的消息队列通过 IPC 标识符区别，不同消息队列直接相互独立。每个消息队列中的消息构成一个独立的链表

> 消息队列克服了信号承载信息量少，管道只能承载无格式字符流
>
> 消息总大小不能超过 8192 个字节

- 本质：消息队列实质上是一个链表，有消息队列标识符(queue ID)。 msgget 创建一个新队列或打开一个存在的队列；msgsnd 向队列末端添加一条新消息；msgrcv 从队列中取消息， 取消息不一定遵循先进先出，也可以按消息的类型字段取消息

- 消息队列与命名管道的比较：

  - 消息队列进行通信的进程可以是不相关的进程，都是通过发送和接收的方式来传递数据的

  - 命名管道发送数据用 write，接收数据用 read，在消息队列中发送数据用 msgsnd，接收数据用 msgrcv，对每个数据都有一个最大长度的限制

  - 与命名管道相比，消息队列的优势在于：

    > 消息队列可独立于发送和接收进程存在，消除了在同步命名管道的打开和关闭时可能产生的困难
    >
    > 通过发送消息可避免命名管道的同步和阻塞问题，不需要由进程自己来提供同步方法
    >
    > 接收程序可以通过消息类型有选择地接收数据，不是像命名管道中那样只能默认地接收

##### （4）信号 signal

> 信号机制是 unix 系统最古老的进程间通信机制，用于一个或几个进程之间传递异步信号。信号可以有各种异步事件产生，如键盘中断等。shell 也可以使用信号将作业控制命令传递给它的子进程

##### （5）信号量 Semaphore

> 信号量是一种计数器，用于控制对多个进程共享的资源进行的访问。常被用作锁机制，在某个进程正在对特定的资源进行操作时，信号量可防止另一个进程去访问它。 信号量是特殊的变量，只取正整数值且只允许对这个值进行两种操作：等待（wait）和信号（signal）（P、V操作，P用于等待，V用于信号） 

- p(sv)：如果 sv 的值大于 0，就给它减 1；如果它的值等于 0，就挂起该进程的执行
- V(sv)：如果有其他进程因等待 sv 而被挂起，就让它恢复运行；如果没有其他进程因等待 sv 而挂起，则给它加 1
- P 相当于申请资源，V 相当于释放资源

##### （6）共享内存

> 共享内存是在多个进程之间共享内存区域的一种进程间的通信方式，由 IPC 为进程创建的一个特殊地址范围。其他进程可以将同一段共享内存连接到自己的地址空间中。所有进程都可以访问共享内存中的地址，如果一个进程向共享内存中写入了数据，所做的改动将立刻被其他进程看到。 

> 共享内存是 IPC 最快捷的方式，没有中间过程，管道、消息队列等方式需要将数据通过中间机制进行转换。共享内存方式直接将某段内存段进行映射，多个进程间的共享内存是同一块的物理空间，仅仅映射到各进程的地址不同而已，因此不需要进行复制，可以直接使用此段空间

> 共享内存本身并没有同步机制，需要程序员自己控制

- 消息队列、信号量、共享内存相似之处：统称为 XSI IPC，在内核中有相似的 IPC 结构（消息队列的 msgid_ds，信号量的 semid_ds，共享内存的 shmid_ds），都用一个非负整数的标识符加以引用（消息队列的 msg_id，信号量的 sem_id，共享内存的 shm_id，分别通过 msgget、semget 以及 shmget 获得），标志符是 IPC 对象的内部名，每个 IPC 对象都有一个键（key_t key）相关联，将这个键作为该对象的外部名
- XSI IPC 和 PIPE、FIFO 的区别：
  - XSI IPC 的 IPC 结构在系统范围内起作用，没用使用引用计数。如果一个进程创建一个消息队列，并在消息队列中放入几个消息，进程终止后，即使现在已经没有程序使用该消息队列，消息队列及其内容依然保留。而 PIPE 在最后一个引用管道的进程终止时，管道就被完全删除。对于 FIFO 最后一个引用 FIFO 的进程终止时，FIFO 还在系统，但其中的内容会被删除
  - 和 PIPE、FIFO 不一样，XSI IPC 不使用文件描述符，不能用 ls 查看 IPC 对象，不能用 rm 命令删除，不能用 chmod 命令删除它们的访问权限。只能使用 ipcs 和 ipcrm 来查看可以删除它们

##### （7）内存映射 mmap()

> 内存映射文件是由一个文件到一块内存的映射。与虚拟内存类似，通过内存映射文件可以保留一个地址的区域，同时将物理存储器提交给此区域，内存文件映射的物理存储器来自一个已经存在于磁盘上的文件，且在对该文件进行操作前必须先对文件进行映射。使用内存映射文件处理存储于磁盘上的文件时，不必再对文件执行 I/O 操作。 每个使用该机制的进程通过把同一个共享的文件映射到自己的进程地址空间来实现多进程间通信（类似共享内存，只要有一个进程对这块映射文件的内存进行操作，其他进程能够马上看到）

> 使用内存映射文件可以实现多进程间通信，还可用于处理大文件提高效率。普通做法是把磁盘上的文件先拷贝到内核空间的一个缓冲区再拷贝到用户空间（内存），用户修改后再将这些数据拷贝到缓冲区再拷贝到磁盘文件，一共四次拷贝。`mmap()` 没有进行数据拷贝，真正的拷贝是在在缺页中断处理时进行的，`mmap()` 将文件直接映射到用户空间，中断处理函数根据这个映射关系，直接将文件从硬盘拷贝到用户空间，只进行一次数据拷贝。效率高于 read/write

- 共享内存和内存映射文件的区别：
  - 内存映射文件是利用虚拟内存把文件映射到进程的地址空间中去，在此之后进程操作文件，就像操作进程空间里的地址一样了，应用在需要频繁处理一个文件或大文件的场合，IO 效率比普通 IO 效率高
  - 共享内存是内存映射文件的一种特殊情况，内存映射的是一块内存，而非磁盘上的文件。共享内存的主语是进程（Process），操作系统默认会给每一个进程分配一个内存空间，每一个进程只允许访问操作系统分配给它的哪一段内存，而不能访问其他进程的。要在不同进程之间访问同一段内存，操作系统给出创建访问共享内存的 API，需要共享内存的进程可以通过 API 来访问，各进程访问这一段内存就像访问一个硬盘上的文件一样
- 内存映射文件与虚拟内存的区别和联系：
  - 联系：虚拟内存和内存映射都是将一部分内容加载到内存，另一部放在磁盘上的一种机制。对用户而言都是透明的
  - 区别：虚拟内存是硬盘的一部分，是内存和硬盘的数据交换区，许多程序运行过程中把暂时不用的程序数据放入这块虚拟内存，节约内存资源。内存映射是一个文件到一块内存的映射，这样程序通过内存指针就可以对文件进行访问
    - 虚拟内存的硬件基础是分页机制。另外一个基础就是局部性原理（时间局部性和空间局部性），这样就可以将程序的一部分装入内存，其余部分留在外存，当访问信息不存在，再将所需数据调入内存。而内存映射文件并不是局部性，而是使虚拟地址空间的某个区域银蛇磁盘的全部或部分内容，通过该区域对被映射的磁盘文件进行访问，不必进行文件 I/O 也不需要对文件内容进行缓冲处理

##### （8）套接字 socket

> 套接字机制不但可以单机的不同进程通信，也可以跨网机器间进程通信

> 套接字的创建和使用与管道有区别，套接字明确将客户端与服务器区分开来，可以实现多个客户端连到同一服务器。

- 服务器套接字连接过程描述： 
  - 服务器应用程序用 socket 创建一个套接字，是系统分配服务器进程的类似文件描述符的资源
  - 服务器调用 bind 给套接字命名。名字是一个标示符，它允许 linux 将进入的针对特定端口的连接转到正确的服务器进程
  - 系统调用 listen 函数开始接听，等待客户端连接。listen 创建一个队列并将其用于存放来自客户端的进入连接
  - 当客户端调用 connect 请求连接时，服务器调用 accept 接受客户端连接，accept 此时会创建一个新套接字，用于与这个客户端进行通信
- 客户端套接字连接过程描述：
  -  客户端调用 socket 创建一个未命名套接字，让后将服务器的命名套接字作为地址来调用 connect 与服务器建立连接
  -  只要双方连接建立成功，可以像操作底层文件一样来操作 socket 套接字实现通信

#### 5、IO多路复用

> 单线程或单进程同时监测若干个文件描述符是否可以执行 IO 操作的能力

- DMA（Direct Memory Access，直接存储器访问）：处理 IO

- Pagecache：Linux 内核所使用的主要磁盘高速缓存。内核读写磁盘时都用到 PageCache

  - 如果程序想读部分不在高速缓存，先申请一个 4KB 大小的新页框加到 PageCache，再用磁盘读到的数据填充

  - 写操作时，先把要写的数据写到 pageCache，标记当前页面为脏，然后程序自己调用系统调用刷盘，或等内核到自己的默认设置刷盘，没及时写时断电白写

- 文件描述符 fd：Linux 将一切抽象为文件，文件描述符用于对应打开/新建的文件，本质是个非负整数。实际上是个索引值，指向内核为每个进程所维护的该进程打开文件的记录表。程序打开现有或创建新文件时，内核向进程返回一个文件描述符

  - 每个进程一旦创建都有三个默认的文件描述符，u 代表读写都可
    - 0u（标准输入）
    - 1u（标准输出）
    - 2u（报错信息输出）

  - 每个文件描述符代表的数据结构中都有自己的偏移量，表示它可从当前文件哪个位置进行操作（读写）
  - 每个进程都有自己的文件描述符，因为进程隔离，不同进程维护的文件描述符可重复
  - 假如不同进程的相同文件描述符指向同一文件，仍各自维护自己的偏移量指针，每个进程可各自访问自己区域

- socket：socket 类型的文件描述符有自己的缓存数据区域，但不是要刷盘的，是要通过网卡发走的，中间经历各种网络协议包装成数据包发往目标 IP 地址

##### （1）select

- 调用过程：
  - 使用copy_from_user从用户空间拷贝fd_set到内核空间
  - 注册回调函数__pollwait
  - 遍历所有fd，调用其对应的poll方法（对于socket，这个poll方法是sock_poll，sock_poll根据情况会调用到tcp_poll,udp_poll或者datagram_poll）
  - 以tcp_poll为例，其核心实现就是__pollwait，也就是上面注册的回调函数
  - __pollwait的主要工作就是把current（当前进程）挂到设备的等待队列中，不同的设备有不同的等待队列，对于tcp_poll来说，其等待队列是sk->sk_sleep（注意把进程挂到等待队列中并不代表进程已经睡眠了）。在设备收到一条消息（网络设备）或填写完文件数据（磁盘设备）后，会唤醒设备等待队列上睡眠的进程，这时current便被唤醒了
  - poll方法返回时会返回一个描述读写操作是否就绪的mask掩码，根据这个mask掩码给fd_set赋值
  - 如果遍历完所有的fd，还没有返回一个可读写的mask掩码，则会调用schedule_timeout是调用select的进程（也就是current）进入睡眠。当设备驱动发生自身资源可读写后，会唤醒其等待队列上睡眠的进程。如果超过一定的超时时间（schedule_timeout指定），还是没人唤醒，则调用select的进程会重新被唤醒获得CPU，进而重新遍历fd，判断有没有就绪的fd
  - 把fd_set从内核空间拷贝到用户空间
- select本质上是通过设置或者检查存放fd标志位的数据结构来进行下一步处理

- 优点：一次系统调用把所有 fds 传给内核，减少 BIO 多次调用的开销（假设 1000 个连接只有一个发来数据，BIO 需向内核发送 1000 次系统调用，999 次无意义，消耗时间和内存资源）
- 缺点
  - 单个进程可监视的fd数量被限制，即能监听端口的大小有限
  - 对socket进行扫描时是线性扫描，即采用轮询的方法，效率较低：
    
    - 每次调用select，都需要把fd集合从用户态拷贝到内核态，这个开销在fd很多时会很大
    - 同时每次调用select都需要在内核遍历传递进来的所有fd，这个开销在fd很多时也很大
  - 需要维护一个用来存放大量fd的数据结构，这样会使得用户空间和内核空间在传递该结构时复制开销大

![img](https://images2018.cnblogs.com/blog/137084/201806/137084-20180611142415772-1018872947.png)

##### （2）poll

- 和select比较：
  - 描述fd集合的方式不同，poll使用pollfd结构，select的fd_set结构
  - 管理多个描述符也是进行轮询
  - 没有最大文件描述符数量的限制
  - 本质和select没有区别，将用户传入的数组拷贝到内核空间，查询每个fd对应的设备状态，如果设备就绪则在设备等待队列中加入一项并继续遍历，如果遍历完所有fd后没有发现就绪设备，则挂起当前进程，直到设备就绪或者主动超时，被唤醒后它又要再次遍历fd。这个过程经历了多次无谓的遍历
  - 没有最大连接数的限制，原因是基于链表来存储

- 缺点
  - 每次 poll 都要重新遍历全量 fds
  - 和select一样，大量的fd的数组被整体复制于用户态和内核地址空间之间，而不管这样的复制是不是有意义
  - “水平触发”，如果报告了fd后，没有被处理，那么下次poll时会再次报告该fd

##### （3）epoll

![Linux_epoll](图片.assets\Linux_epoll.png)

- Linux 特有
- 使用“事件”的就绪通知方式，通过epoll_ctl注册fd，一旦该fd就绪，内核就会采用类似callback的回调机制来激活该fd，epoll_wait便可以收到通知

- 工作模式
  - LT
    - fd 可读后，如果服务程序读走一部分就结束此次读取，LT 模式下该文件描述符仍然可读
    - fd 可写后，如果服务程序写了一部分就结束此次写入，LT 模式下该文件描述符仍然可写
  - ET
    - fd 可读后，如果服务程序读走一部分就结束此次读取，ET 模式下该文件描述符不可读，需等到下次数据到达时才变为可读，要保证循环读取数据，确保把所有数据读出
    - fd 可写后，如果服务程序写了一部分就结束此次写入，ET 模式下该文件描述符不可写，要写入数据，确保把数据写满
  
- 优点：

  - 没有最大并发连接的限制，能打开的FD的上限远大于1024（1G的内存上能监听约10万个端口）
  - 效率提升，不是轮询的方式，不会随着FD数目的增加效率下降。只有活跃可用的FD才会调用callback函数
  - 只管“活跃”的连接，跟连接总数无关，在实际的网络环境中，效率就会远远高于select和poll
  - 内存拷贝：利用mmap()文件映射内存加速与内核空间的消息传递，epoll使用mmap减少复制开销

- 相比select、poll：

  - 调用接口上，select和poll都只提供了一个函数——select或者poll函数。而epoll提供了三个函数，epoll_create（创建一个epoll句柄）、epoll_ctl（注册要监听的事件类型）、epoll_wait（等待事件的产生）
    - epoll_ctl函数：每次注册新的事件到epoll句柄中时（在epoll_ctl中指定EPOLL_CTL_ADD），把所有的fd拷贝进内核，而不是在epoll_wait的时候重复拷贝。保证了每个fd在整个过程中只会拷贝一次
    - 不像select或poll一样每次都把current轮流加入fd对应的设备等待队列中，而只在epoll_ctl时把current挂一遍（这一遍必不可少）并为每个fd指定一个回调函数，当设备就绪，唤醒等待队列上的等待者时，调用这个回调函数，回调函数会把就绪的fd加入一个就绪链表。epoll_wait在这个就绪链表中查看有没有就绪的fd（利用schedule_timeout()实现睡一会，判断一会的效果，和select实现中的第7步是类似的）
    - 没有FD个数限制

- 机制

  - 设计：在Linux内核中申请一个简易的文件系统(文件系统一般用B+树)

    - 调用epoll_create() 建立一个epoll对象(在epoll文件系统中为这个句柄对象分配资源)
    - 调用epoll_ctl向epoll对象中添加连接的套接字
    - 调用epoll_wait收集      发生的事件的连接

  - 当某一进程调用epoll_create方法时，Linux内核会创建一个eventpoll结构体，这个结构体中有两个成员与epoll的使用方式密切相关。eventpoll结构体：

    ```c
    struct eventpoll{
        ....
        /*红黑树的根节点，这颗树中存储着所有添加到epoll中的需要监控的事件*/
        struct rb_root  rbr;
        /*双链表中则存放着将要通过epoll_wait返回给用户的满足条件的事件*/
        struct list_head rdlist;
        ....
    };
    ```

  - 每一个epoll对象都有一个独立的eventpoll结构体，用于存放通过epoll_ctl方法向epoll对象中添加进来的事件。这些事件都会挂载在红黑树中，如此，重复添加的事件就可以通过红黑树而高效的识别出来(红黑树的插入时间效率是lgn，其中n为树的高度)

  - 所有添加到epoll中的事件都会与设备(网卡)驱动程序建立回调关系，也就是说，当相应的事件发生时会调用这个回调方法。这个回调方法在内核中叫ep_poll_callback，它会将发生的事件添加到rdlist双链表中

  - 对于每一个事件，都会建立一个epitem结构体，如下所示：

    ```c
    struct epitem{
        struct rb_node  rbn;//红黑树节点
        struct list_head    rdllink;//双向链表节点
        struct epoll_filefd  ffd;  //事件句柄信息
        struct eventpoll *ep;    //指向其所属的eventpoll对象
        struct epoll_event event; //期待发生的事件类型
    }
    ```

  - 当调用epoll_wait检查是否有事件发生时，只需要检查eventpoll对象中的rdlist双链表中是否有epitem元素即可。如果rdlist不为空，则把发生的事件复制到用户态，同时将事件数量返回给用户

  ![img](https://img-blog.csdnimg.cn/20190518113809289.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3d0ZXJ1aXljYnFxdnd0,size_16,color_FFFFFF,t_70)

- 调用

  - epoll_create()系统调用。此调用返回一个句柄，之后所有的使用都依靠这个句柄来标识
  - epoll_ctl()系统调用。通过此调用向epoll对象中添加、删除、修改感兴趣的事件，返回0标识成功，返回-1表示失败
  - epoll_wait()系统调用。通过此调用收集在epoll监控中已经发生的事件

##### （4）对比

- 支持一个进程所能打开的最大连接数

  - select：单个进程所能打开的最大连接数有FD_SETSIZE宏定义，其大小是32个整数的大小（在32位的机器上，大小就是3232，同理64位机器上FD_SETSIZE为3264）
  - poll：没有最大连接数的限制，原因是基于链表来存储的
  - epoll：有上限，但是很大，1G内存的机器上可以打开10万左右的连接

- FD剧增后带来的IO效率问题

  - select：因为每次调用时都会对连接进行线性遍历，所以随着FD的增加会造成遍历速度慢的“线性下降性能问题”
  - poll：同上
  - epoll：因为epoll内核中实现是根据每个fd上的callback函数来实现的，只有活跃的socket才会主动调用callback，所以在活跃socket较少的情况下，使用epoll没有前面两者的线性下降的性能问题，但是所有socket都很活跃的情况下，可能会有性能问题

- 消息传递方式

  - select：内核需要将消息传递到用户空间，都需要内核拷贝动作

  - poll：同上

  - epoll：epoll通过内核和用户空间共享一块内存来实现的

- 总结：
  - 表面上看epoll的性能最好，但是在连接数少并且连接都十分活跃的情况下，select和poll的性能可能比epoll好，毕竟epoll的通知机制需要很多函数回调
    - select，poll实现需要自己不断轮询所有fd集合，直到设备就绪，期间可能要睡眠和唤醒多次交替。而epoll其实也需要调用epoll_wait不断轮询就绪链表，期间也可能多次睡眠和唤醒交替，但是它是设备就绪时，调用回调函数，把就绪fd放入就绪链表中，并唤醒在epoll_wait中进入睡眠的进程。虽然都要睡眠和交替，但是select和poll在“醒着”的时候要遍历整个fd集合，而epoll在“醒着”的时候只要判断一下就绪链表是否为空就行了，这节省了大量的CPU时间。这就是回调机制带来的性能提升
  - select低效是因为每次它都需要轮询。但低效也是相对的，视情况而定
    - select，poll每次调用都要把fd集合从用户态往内核态拷贝一次，且要把current往设备等待队列中挂一次，而epoll只要一次拷贝，而且把current往等待队列上挂也只挂一次（在epoll_wait的开始，注意这里的等待队列并不是设备等待队列，只是一个epoll内部定义的等待队列），节省不少的开销

1、表面上看epoll的性能最好，但是在连接数少并且连接都十分活跃的情况下，select和poll的性能可能比epoll好，毕竟epoll的通知机制需要很多函数回调。



