### Linux

####1、xshell

> 上传文件到服务器：`rz -y`
>
> 下载文件到本地：`sz + 文件`
>
> 获得权限：`sudo su` 回车后输入密码

#### 2、常用命令

>`ls` 列出文件夹里文件名称
>
>`ll` 查看文件夹信息，文件总数等详细信息
>
>`cd /` 回到主文件夹；`cd ..` 返回上一级目录
>
>`rm +文件` 删除文件（`-f` 强制删除）
>
>`rm -r +文件夹` 删除文件夹
>
>`mkdir +文件夹` 创建文件夹
>
>`mv 文件名 目标文件夹目录(例如/home/USER_zy)` 移动文件到目标文件夹
>
>`ps -ef|grep py` 查看当前进程
>
>`df -hl` 查看内存空间

####3、VI编辑

> 按ESC进入Command模式，输入
>
> > `：wq` 回车就是保存退出
> >
> > `w` 保存
> >
> > `wq!` 保存强制退出
> >
> > `q!` 强制退出

#### 4、Linux IPC 机制：

##### （1）管道 PIPE

> 实际是用于进程间通信的一段共享内存，创建管道的进程称为管道服务器，连接到一个管道的进程为管道客户机。一个进程在向管道写入数据后，另一进程就可以从管道的另一端将其读取出来

- 特点：

  - 半双工，需要双方通信时，要建立起两个管道
  - 只能用于父子进程或兄弟进程之间（具有亲缘关系的进程）。如 fork 或 exec 创建的新进程，使用 exec 创建新进程时，需要将管道的文件描述符作为参数传递给 exec 创建的新进程。 当父进程与 fork 创建的子进程直接通信时，发送数据的进程关闭读端，接受数据的进程关闭写端
  - 单独构成一种独立的文件系统：管道对于管道两端的进程而言是一个文件，但不属于某种文件系统，而是单独构成一种文件系统，且只存在与内存中
  - 数据的读出和写入：一个进程向管道中写的内容被管道另一端的进程读出。写入的内容每次都添加在管道缓冲区的末尾，且每次都从缓冲区的头部读出数据

- 实现机制：管道是由内核管理的一个缓冲区，一个缓冲区不需要很大，它被设计成为环形的数据结构，以便管道可以被循环利用。当管道中没有信息的话，从管道中读取的进程会等待，直到另一端的进程放入信息。当管道被放满信息的时候，尝试放入信息的进程会等待，直到另一端的进程取出信息。当两个进程都终结的时候，管道也自动消失。

- pipe 函数原型：通过使用底层的 read 和 write 调用来访问数据。 向 file_descriptor[1] 写数据，从 file_descriptor[0] 读数据。写入与读取原则是先进先出

- 读写规则

  - 当没有数据可读时

    > O_NONBLOCK disable：read 调用阻塞，即进程暂停执行，等到有数据来到为止
    >
    > O_NONBLOCK enable：read 调用返回 -1，errno 值为 EAGAIN

  - 当管道满时

    > O_NONBLOCK disable： write 调用阻塞，直到有进程读走数据
    > O_NONBLOCK enable：调用返回 -1，errno 值为 EAGAIN

  - 如果所有管道写端对应的文件描述符被关闭：read 返回 0

  - 如果所有管道读端对应的文件描述符被关闭：write 操作会产生信号 SIGPIPE

  - 当要写入的数据量不大于 PIPE_BUF（Posix.1 要求 PIPE_BUF 至少 512 字节）时，linux 将保证写入的原子性

  - 当要写入的数据量大于 PIPE_BUF 时，linux 不再保证写入的原子性

##### （2）命名管道 FIFO

> 是一种特殊类型的文件，在系统中以文件形式存在。克服了管道的弊端，允许没有亲缘关系的进程间通信

- 管道和命名管道的区别：
  - 对于命名管道 FIFO 来说，IO 操作和普通管道 IO 操作基本一样，主要区别是在命名管道中，管道可以是事先已经创建好的，如在命令行下执行 `mkfifo myfifo` 创建一个命名通道，必须用 open 函数显示建立连接到管道的通道，而在管道中，管道已经在主进程里创建好了，在 fork 时直接复制相关数据或用 exec 创建的新进程时把管道的文件描述符当参数传递进去
  - 一般来说 FIFO 和 PIPE 一样总处于阻塞状态。如果命名管道 FIFO 打开时设置了读权限，则读进程将一直阻塞，直到其他进程打开该 FIFO 并向管道写入数据。这个阻塞动作反过来成立。在 open 时使用 O_NONBLOCK 标志可以关闭默认的阻塞操作

##### （3）消息队列

> 消息队列是内核地址空间中的内部链表，通过 linux 内核在各个进程直接传递内容，消息顺序地发送到消息队列中，并以几种不同的方式从队列中获得，每个消息队列可以用 IPC 标识符唯一地进行识别。内核中的消息队列通过 IPC 标识符区别，不同消息队列直接相互独立。每个消息队列中的消息构成一个独立的链表

> 消息队列克服了信号承载信息量少，管道只能承载无格式字符流
>
> 消息总大小不能超过 8192 个字节

- 本质：消息队列实质上是一个链表，有消息队列标识符(queue ID)。 msgget 创建一个新队列或打开一个存在的队列；msgsnd 向队列末端添加一条新消息；msgrcv 从队列中取消息， 取消息不一定遵循先进先出，也可以按消息的类型字段取消息

- 消息队列与命名管道的比较：

  - 消息队列进行通信的进程可以是不相关的进程，都是通过发送和接收的方式来传递数据的

  - 命名管道发送数据用 write，接收数据用 read，在消息队列中发送数据用 msgsnd，接收数据用 msgrcv，对每个数据都有一个最大长度的限制

  - 与命名管道相比，消息队列的优势在于：

    > 消息队列可独立于发送和接收进程存在，消除了在同步命名管道的打开和关闭时可能产生的困难
    >
    > 通过发送消息可避免命名管道的同步和阻塞问题，不需要由进程自己来提供同步方法
    >
    > 接收程序可以通过消息类型有选择地接收数据，不是像命名管道中那样只能默认地接收

##### （4）信号 signal

> 信号机制是 unix 系统最古老的进程间通信机制，用于一个或几个进程之间传递异步信号。信号可以有各种异步事件产生，如键盘中断等。shell 也可以使用信号将作业控制命令传递给它的子进程

##### （5）信号量 Semaphore

> 信号量是一种计数器，用于控制对多个进程共享的资源进行的访问。常被用作锁机制，在某个进程正在对特定的资源进行操作时，信号量可防止另一个进程去访问它。 信号量是特殊的变量，只取正整数值且只允许对这个值进行两种操作：等待（wait）和信号（signal）（P、V操作，P用于等待，V用于信号） 

- p(sv)：如果 sv 的值大于 0，就给它减 1；如果它的值等于 0，就挂起该进程的执行
- V(sv)：如果有其他进程因等待 sv 而被挂起，就让它恢复运行；如果没有其他进程因等待 sv 而挂起，则给它加 1
- P 相当于申请资源，V 相当于释放资源

##### （6）共享内存

> 共享内存是在多个进程之间共享内存区域的一种进程间的通信方式，由 IPC 为进程创建的一个特殊地址范围。其他进程可以将同一段共享内存连接到自己的地址空间中。所有进程都可以访问共享内存中的地址，如果一个进程向共享内存中写入了数据，所做的改动将立刻被其他进程看到。 

> 共享内存是 IPC 最快捷的方式，没有中间过程，管道、消息队列等方式需要将数据通过中间机制进行转换。共享内存方式直接将某段内存段进行映射，多个进程间的共享内存是同一块的物理空间，仅仅映射到各进程的地址不同而已，因此不需要进行复制，可以直接使用此段空间

> 共享内存本身并没有同步机制，需要程序员自己控制

- 消息队列、信号量、共享内存相似之处：统称为 XSI IPC，在内核中有相似的 IPC 结构（消息队列的 msgid_ds，信号量的 semid_ds，共享内存的 shmid_ds），都用一个非负整数的标识符加以引用（消息队列的 msg_id，信号量的 sem_id，共享内存的 shm_id，分别通过 msgget、semget 以及 shmget 获得），标志符是 IPC 对象的内部名，每个 IPC 对象都有一个键（key_t key）相关联，将这个键作为该对象的外部名
- XSI IPC 和 PIPE、FIFO 的区别：
  - XSI IPC 的 IPC 结构在系统范围内起作用，没用使用引用计数。如果一个进程创建一个消息队列，并在消息队列中放入几个消息，进程终止后，即使现在已经没有程序使用该消息队列，消息队列及其内容依然保留。而 PIPE 在最后一个引用管道的进程终止时，管道就被完全删除。对于 FIFO 最后一个引用 FIFO 的进程终止时，FIFO 还在系统，但其中的内容会被删除
  - 和 PIPE、FIFO 不一样，XSI IPC 不使用文件描述符，不能用 ls 查看 IPC 对象，不能用 rm 命令删除，不能用 chmod 命令删除它们的访问权限。只能使用 ipcs 和 ipcrm 来查看可以删除它们

##### （7）内存映射 mmap()

> 内存映射文件是由一个文件到一块内存的映射。与虚拟内存类似，通过内存映射文件可以保留一个地址的区域，同时将物理存储器提交给此区域，内存文件映射的物理存储器来自一个已经存在于磁盘上的文件，且在对该文件进行操作前必须先对文件进行映射。使用内存映射文件处理存储于磁盘上的文件时，不必再对文件执行 I/O 操作。 每个使用该机制的进程通过把同一个共享的文件映射到自己的进程地址空间来实现多进程间通信（类似共享内存，只要有一个进程对这块映射文件的内存进行操作，其他进程能够马上看到）

> 使用内存映射文件可以实现多进程间通信，还可用于处理大文件提高效率。普通做法是把磁盘上的文件先拷贝到内核空间的一个缓冲区再拷贝到用户空间（内存），用户修改后再将这些数据拷贝到缓冲区再拷贝到磁盘文件，一共四次拷贝。`mmap()` 没有进行数据拷贝，真正的拷贝是在在缺页中断处理时进行的，`mmap()` 将文件直接映射到用户空间，中断处理函数根据这个映射关系，直接将文件从硬盘拷贝到用户空间，只进行一次数据拷贝。效率高于 read/write

- 共享内存和内存映射文件的区别：
  - 内存映射文件是利用虚拟内存把文件映射到进程的地址空间中去，在此之后进程操作文件，就像操作进程空间里的地址一样了，应用在需要频繁处理一个文件或大文件的场合，IO 效率比普通 IO 效率高
  - 共享内存是内存映射文件的一种特殊情况，内存映射的是一块内存，而非磁盘上的文件。共享内存的主语是进程（Process），操作系统默认会给每一个进程分配一个内存空间，每一个进程只允许访问操作系统分配给它的哪一段内存，而不能访问其他进程的。要在不同进程之间访问同一段内存，操作系统给出创建访问共享内存的 API，需要共享内存的进程可以通过 API 来访问，各进程访问这一段内存就像访问一个硬盘上的文件一样
- 内存映射文件与虚拟内存的区别和联系：
  - 联系：虚拟内存和内存映射都是将一部分内容加载到内存，另一部放在磁盘上的一种机制。对用户而言都是透明的
  - 区别：虚拟内存是硬盘的一部分，是内存和硬盘的数据交换区，许多程序运行过程中把暂时不用的程序数据放入这块虚拟内存，节约内存资源。内存映射是一个文件到一块内存的映射，这样程序通过内存指针就可以对文件进行访问
    - 虚拟内存的硬件基础是分页机制。另外一个基础就是局部性原理（时间局部性和空间局部性），这样就可以将程序的一部分装入内存，其余部分留在外存，当访问信息不存在，再将所需数据调入内存。而内存映射文件并不是局部性，而是使虚拟地址空间的某个区域银蛇磁盘的全部或部分内容，通过该区域对被映射的磁盘文件进行访问，不必进行文件 I/O 也不需要对文件内容进行缓冲处理

##### （8）套接字 socket

> 套接字机制不但可以单机的不同进程通信，也可以跨网机器间进程通信

> 套接字的创建和使用与管道有区别，套接字明确将客户端与服务器区分开来，可以实现多个客户端连到同一服务器。

- 服务器套接字连接过程描述： 
  - 服务器应用程序用 socket 创建一个套接字，是系统分配服务器进程的类似文件描述符的资源
  - 服务器调用 bind 给套接字命名。名字是一个标示符，它允许 linux 将进入的针对特定端口的连接转到正确的服务器进程
  - 系统调用 listen 函数开始接听，等待客户端连接。listen 创建一个队列并将其用于存放来自客户端的进入连接
  - 当客户端调用 connect 请求连接时，服务器调用 accept 接受客户端连接，accept 此时会创建一个新套接字，用于与这个客户端进行通信
- 客户端套接字连接过程描述：
  -  客户端调用 socket 创建一个未命名套接字，让后将服务器的命名套接字作为地址来调用 connect 与服务器建立连接
  -  只要双方连接建立成功，可以像操作底层文件一样来操作 socket 套接字实现通信

#### 5、IO多路复用

> 单线程或单进程同时监测若干个文件描述符是否可以执行 IO 操作的能力

- DMA（Direct Memory Access，直接存储器访问）：处理 IO

- Pagecache：Linux 内核所使用的主要磁盘高速缓存。内核读写磁盘时都用到 PageCache

  - 如果程序想读部分不在高速缓存，先申请一个 4KB 大小的新页框加到 PageCache，再用磁盘读到的数据填充

  - 写操作时，先把要写的数据写到 pageCache，标记当前页面为脏，然后程序自己调用系统调用刷盘，或等内核到自己的默认设置刷盘，没及时写时断电白写

- 文件描述符 fd：Linux 将一切抽象为文件，文件描述符用于对应打开/新建的文件，本质是个非负整数。实际上是个索引值，指向内核为每个进程所维护的该进程打开文件的记录表。程序打开现有或创建新文件时，内核向进程返回一个文件描述符

  - 每个进程一旦创建都有三个默认的文件描述符，u 代表读写都可
    - 0u（标准输入）
    - 1u（标准输出）
    - 2u（报错信息输出）

  - 每个文件描述符代表的数据结构中都有自己的偏移量，表示它可从当前文件哪个位置进行操作（读写）
  - 每个进程都有自己的文件描述符，因为进程隔离，不同进程维护的文件描述符可重复
  - 假如不同进程的相同文件描述符指向同一文件，仍各自维护自己的偏移量指针，每个进程可各自访问自己区域

- socket：socket 类型的文件描述符有自己的缓存数据区域，但不是要刷盘的，是要通过网卡发走的，中间经历各种网络协议包装成数据包发往目标 IP 地址

##### （1）select

- 把 readset 和 writeset 的文件描述符对应位置 1，交给 `select()` 系统调用判断哪个文件描述符打开（将读/写文件描述符集合从用户态拷贝到内核态中，在内核中判断事件来临，会阻塞），如果某个 fd 不可读/写，该位置 0，如果可读，数组中仍然为 1，系统调用后返回可读可写的数量和
- 循环遍历所有文件描述符用 FD_ISSET() 判断是否进行读写操作
- 有数据来临，FD 置位（不可重用），select 返回（不阻塞）

```c
int FD_ISSET(int fd,fd_set *fdset);  //返回值：若fd在文件描述符集中，返回非0值；否则，返回0
void FD_CLR(int fd,fd_set *fdset);   //清除最后一位
void FD_SET(int fd,fd_set *fdset);   //开启描述符中的一位
void FD_ZERO(fd_set *fdset);         //所有描述符位置位0

while(1){
    FD_ZERO(&rset);
    for(i=0;i<5;i+r){
        //&rset是文件描述符集合
        //如五个文件描述符存的12346，则&rset为0111101（有1024位）
	    FD_SET(fds[i],&rset);
    }
	//文件描述符最大值+1，读文件描述符集合，写文件描述符集合，异常文件描述符集合，超时时间
    //int select(int maxfpd1,fdset *read_fds,fdset *write_fds,fdset *exception_fds,struct timeval *restrict tvpr);
    select(max+1,&rset,NULL,NULL,NULL);//会阻塞
    for(i=0;i<5;i++){//再遍历
	    if(FD_ISSET(fds[i],&rset)){
		    memset(buffer,0,MAXBUF);
		    read(fds[i],buffer,MAXBUF);
            puts(buffer);
    }
}
```

- 优点：一次系统调用把所有 fds 传给内核，减少 BIO 多次调用的开销（假设 1000 个连接只有一个发来数据，BIO 需向内核发送 1000 次系统调用，999 次无意义，消耗时间和内存资源）
- 缺点
  - 最大文件描述符编号为 1024
  - 直接在 readset、writeset 做修改，不可重用
  - 用户态到内核态的切换开销
  - 不能返回哪位有事件，需再遍历

##### （2）poll

- 将rset从用户态拷贝到内核态中，在内核中判断事件来临（会阻塞）
- 有数据来临，pollfd.revents置位，poll返回（不阻塞）

```c
struct pollfd{
	int fd;
	short events;//在意的事件（POLLIN读、POLLOUT写）
    short revents;//events的回馈，默认0
}

while(1){
    puts("round again");
	//pollfd数组，元素个数，超时时间
    //int poll(struct pollfd fdarry[],nfds_t nfds,int timeout);
    poll(pollfds,5,50000);//会阻塞
    for(i=0;i<5;i++){//再遍历
	    if(pollfds[i].revents & POLLIN){
		    pollfds[i].revents = 0;//置0，可重用
		    memset(buffer,0,MAXBUF);
		    read(pollfds[i].fd,buffer,MAXBUF);
            puts(buffer);
        }
    }
}
```

- 优点
  - 内核操作的是结构体的 revents 字段，没有破坏其他字段，可复用
  - 没有 select 最大支持 1024 个文件描述符的限制
- 缺点
  - 每次 poll 都要重新遍历全量 fds
  - 不能返回哪位有事件，需再遍历

##### （3）epoll

![Linux_epoll](图片.assets\Linux_epoll.png)

- Linux 特有
- 把文件描述符放到内核一个事件表中，epoll 需用一个额外文件描述符表示内核中的事件表

```c
struct epoll_event{
    _uint32_t events; //epoll事件，读、写、异常三种
    epoll_data_t data; //用户数据
}
struct epoll_data{
    void* prt;
    int fd;
    _uint32_t u32;
    _uint64_t u64;
}epoll_data_t;

int epoll_create(int size);//返回一个文件描述符，描述的是内核中一块内存区域，size现在不起作用
//用来操作内核事件表，epfd表示epoll_create()返回的事件表，fd表示新创建的socket文件描述符
//op:
//EPOLL_CTL_ADD：事件表中添加一个文件描述符，内核应关注的socket事件在epoll_event结构体中，添加到事件表的文件描述符以红黑树形式存在，防止重复添加
//EPOLL_CTL_MOD：修改fd上注册的事件
//EPOLL_CTL_DEL：删除fd上注册的事件
int epoll_ctl(int epfd,int op,int fd,struct epoll_event *event);
int epoll_wait(int epfd,struct epoll_event * events,int maxevents,int timeout);//返回就绪文件描述符个数
```

- 工作模式
  - LT
    - fd 可读后，如果服务程序读走一部分就结束此次读取，LT 模式下该文件描述符仍然可读
    - fd 可写后，如果服务程序写了一部分就结束此次写入，LT 模式下该文件描述符仍然可写
  - ET
    - fd 可读后，如果服务程序读走一部分就结束此次读取，ET 模式下该文件描述符不可读，需等到下次数据到达时才变为可读，要保证循环读取数据，确保把所有数据读出
    - fd 可写后，如果服务程序写了一部分就结束此次写入，ET 模式下该文件描述符不可写，要写入数据，确保把数据写满