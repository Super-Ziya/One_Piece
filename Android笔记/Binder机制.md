### Binder机制

> 进程间内存空间是进程私有的
>
> 进程间和内核的空间是互通的
>
> Binder 跨进程通信（IPC）原理就是利用这个可共享的内核空间将进程私有的空间进行关联

#### 1、Android 与 Linux 通信机制

- Android 使用 Linux 内核
- Android 用 Binder 取代 Linux IPC 通信机制
  - Binder 实现 IBinder 接口，采用 C/S 通信模式，安全性高（传统 IPC 没有安全措施），UID/PID 由 Binder 机制在内核空间添加身份标识（传统 IPC 的接收方无法获得对方进程可靠的 UID 和 PID），可建立私有通道（传统是开放的），1 次拷贝
  - socket 是通用接口，传输效率低，开销大
  - 管道和消息队列采用传承转发，至少需要拷贝 2 次数据，效率低
  - 共享内存：没有拷贝数据，但控制机制复杂（跨进程需获取对方 PID，多种机制协同操作）
- Binder 通信整体框架
  - Client、Server 存在用户空间，通信由 Binder 驱动内核实现
  - Binder 驱动：软件，实现方式和设备驱动程序一样，工作于内核态，负责进程 Binder 通信的建立，Binder 的传递，引用计数管理等底层支持
  - ServiceManager（SM）：守护进程
    - 作用：将字符形式的 Binder 名字转化成 Client 中对该 Binder 的引用，使 Client 能通过 Binder 名字获得 Server 中 Binder 实体的引用

<img src="Image.assets\Binder_整体框架.jpg" alt="Binder_整体框架" style="zoom:80%;" />

- 匿名 Binder

  - Server 可通过已建立的 Binder 连接将创建的 Binder 实体传给 Client，Binder 连接必须通过实名 Binder实现。由于这个Binder 没有向 SM 注册名字，是个匿名 Binder。Client 会收到匿名 Binder 的引用，通过引用向位于 Server 中的实体发送请求。匿名 Binder 为通信双方建立一条私密通道

- 优点

  - 高效：
    - Binder 数据拷贝只需要 1 次，而管道、消息队列、Socket 都需 2 次
    - 通过驱动在内核空间拷贝数据，不需额外同步处理
  - 安全性高
    Binder 机制为每个进程分配了 UID/PID 作为鉴别身份的标示，在 Binder 通信时根据 UID/PID 进行有效性检测
    - 传统进程通信方式对于通信双方的身份没有严格验证，如 Socket 通信 ip 地址是客户端手动填入，易出现伪造

  - 使用简单
    - 采用 Client/Server 架构
    - 实现面向对象的调用方式，即使用 Binder 时和调用一个本地对象实例一样

#### 2、使用步骤：

> 代码以客户端委托服务端加法运算后返回为例

- 服务端注册服务

  - XXserver 在自己进程中向 Binder 驱动申请创建一个 XXservice 的 Binder 实体
  - Binder 驱动为 XXservice 创建位于内核中的 Binder 实体节点以及 Binder 引用，将名字 XX 与引用打包传递给 SM，通知 SM 注册一个名为 XX 的 service
  - SM 收到数据包后，解析出名字 XX 和引用，填入一张查找表中（Client 向 SM 发送申请服务 XXService 的请求，SM 在查找表中找到该 Service 的 Binder 引用，把 Binder 引用返回给 Client）

  ![Binder_注册服务](Image.assets\Binder_注册服务.png)

- 获取 SM 远程接口

  - SM 和 Server 都是进程，Server 向 SM 注册也需要进程间通信，Server 端拥有 Binder 的实体，Client 端拥有 Binder 的引用
  - 把 SM 看作 Server 端，在 Binder 驱动一开始运行时就有 Binder 实体（设置 SM 的 Binder 的 handle 值恒为 0）。这个 Binder 实体没有名字也不需注册，所有 client 以 handle 为 0 的 binder 引用与 SM 通信
  - SM 的 handle 为 0 的实体：一个进程调用 Binder 驱动时，使用 BINDER_SET_CONTEXT_MGR 命令（在驱动的 binder_ioctl 中）将自己注册成 SM 时，Binder 驱动会自动为它创建 Binder 实体，这个 Binder 引用对所有 Client 都为 0

  ![Binder_SM注册](Image.assets\Binder_SM注册.jpg)

- 客户端获取服务

  - Client 通过 Service 名字在 SM 的查找表中获得该 Binder 的引用：

    - Client 利用保留的 handle 值为 0 的引用向 SM 请求
    - SM 从请求数据包中获得 Service 名字，在查找表中找到该名字对应条目，取出 Binder 引用打包回复给 client

    ![Binder_获取服务](Image.assets\Binder_获取服务.png)

#### 3、Binder 内存映射与接收缓存区管理

- Binder 驱动管理数据接收缓存，Binder 驱动实现 mmap() 系统调用，mmap() 通常用在有物理存储介质的文件系统上，而 Binder 没有物理介质，而是用来创建数据接收的缓存空间

  - mmap()：

  ```java
  fd = open("/dev/binder", O_RDWR);
  mmap(NULL, MAP_SIZE, PROT_READ, MAP_PRIVATE, fd, 0);
  ```

  - Binder 接收方有一片 MAP_SIZE 的接收缓存区。mmap() 返回值是内存映射在用户空间的地址，这段空间由驱动管理，用户不能直接访问（映射类型为 PROT_READ，只读映射）

  - 接收数据包结构消息头 binder_transaction_data，有效负荷位于 data.buffer 指向的内存中。这片内存不需接收方提供，是 mmap() 映射的缓存池。数据从发送方向接收方拷贝时，驱动根据发送数据包大小，使用最佳匹配算法从缓存池中找到一块大小合适的空间，将数据从发送缓存区复制过来
  - 接收方处理完数据包后通知驱动释放 data.buffer 指向的内存区，由命令 BC_FREE_BUFFER 完成

  - 效率上 mmap() 分配的内存映射在接收方用户空间里，省去内核中暂存，提升一倍性能
  - Linux 内核没有从一个用户空间到另一个用户空间直接拷贝的函数，需先用 copy_from_user() 拷贝到内核空间，再用 copy_to_user() 拷贝到另一个用户空间。mmap() 分配的内存除了映射进接收方进程里，还映射进内核空间，所以调用 copy_from_user() 将数据拷贝进内核空间相当拷贝进接收方用户空间（1 次拷贝）

#### 4、mmap

> Linux 使用 mmap 在进程虚拟内存地址空间中分配地址空间，创建和物理内存的映射关系

![Binder_mmap](Image.assets\Binder_mmap.jpg)

- 映射分类：（4 种组合）

  > mmap 只在虚拟内存分配地址空间，第一次访问虚拟内存时才分配物理内存

  - 文件映射：磁盘文件映射进程的虚拟地址空间，使用文件内容初始化物理内存
  - 匿名映射：初始化全为 0 的内存空间
  - 私有映射（MAP_PRIVATE）：多进程间数据共享，修改不反映到磁盘实际文件，copy-on-write（写时复制）的映射方式
  - 共享映射（MAP_SHARED）：多进程间数据共享，修改反应到磁盘实际文件中

- write

  - 进程（用户态）将需写入的数据直接 copy 到对应 mmap 地址（内存 copy）
  - 若 mmap 地址未对应物理内存（第一次访问），产生缺页异常，由内核处理
  - 若已对应直接 copy 到对应物理内存
  - 操作系统调用，将脏页回写到磁盘（通常异步）

  > 物理内存有限，mmap 写入数据超过物理内存时，操作系统会进行页置换，根据淘汰算法，将需要淘汰的页置换成所需的新页，mmap 对应内存是可被淘汰的（若内存页是"脏"的，操作系统先将数据回写磁盘再淘汰）。就算 mmap 数据远大于物理内存，操作系统也能很好地处理

- read

  - mmap 比普通 read 系统调用少一次 copy。read 调用，进程无法直接访问 kernel space（内核空间），read 系统调用返回前内核需要将数据从内核复制到进程指定的 buffer。mmap 后进程可直接访问 mmap 的数据（page cache）

  ![Binder_mmap_read](Image.assets\Binder_mmap_read.png)

- 优点

  - 对文件读取操作跨过页缓存，减少了数据拷贝次数，用内存读写取代 I/O 读写，提高文件读取效率
  - 实现用户空间和内核空间的高效交互。两空间各自修改操作可直接反映在映射区域内，被对方空间及时捕捉
  - 提供进程间共享内存及相互通信的方式。进程与亲缘无关，通过各自对映射区域的改动，达到进程间通信和共享目的。如果进程 A、B 都映射区域 C，当 A 第一次读取 C 时通过缺页从磁盘复制文件页到内存中，当 B 再读 C 相同页面时也会产生缺页异常，但不需从磁盘中复制文件，可直接使用已经保存在内存中的文件数据
  - 可用于实现高效的大规模数据传输，需要用磁盘空间代替内存时，mmap 效率较高

- 缺点

  - 文件如果很小，由于内存最小粒度是页，进程虚拟地址空间和内存映射以页为单位。如果被映射的文件很小，但对应到进程虚拟地址区域的大小需要满足整页大小，因此 mmap 函数执行后，实际映射到虚拟内存区域高字节部分用零填充，浪费内存空间
  - 对变长文件不适合，文件无法完成拓展，mmap 到内存时操作范围确定
  - 如果更新文件操作很多，会触发大量脏页回写及由此引发的随机 IO 上，效率降低

#### 5、Binder 接收线程管理

- Binder 通信实际是不同进程中线程间的通信，Server 端处理请求的线程相当于 Client 端请求线程在 Server 端中的代理，驱动会将 Client 端线程的优先级赋给 Server 端线程，这样两者会用类似时间完成任务

- Binder 管理线程池

  > 一个进程的 Binder 线程数默认最大 16，超过阻塞

  - 命令
    - INDER_SET_MAX_THREADS：应用程序通过该命令告知驱动最多可创建多少线程
    - BC_REGISTER_LOOP：线程创建用该命令告知驱动
    - BC_ENTER_LOOP：线程进入主循环用该命令告知驱动
    - BC_EXIT_LOOP：线程退出主循环用该命令告知驱动
    - BR_SPAWN_LOOPER：驱动接收完数据包返回读 Binder 线程时，检查是否没有闲置线程，如果是，且线程总数不超出线程池最大线程数，就在当前读出的数据包后面追加一条该命令告诉用户线程不够，新线程一启动通过 BC_xxx_LOOP 告知驱动更新状态，只要线程没耗尽，总有空闲线程在等待队列中及时处理请求
  - 优化：当进程 P1 的线程 T1 向进程 P2 发送请求时，驱动会先查看一下线程 T1 是否也正在处理来自 P2 某线程请求但未完成（没有发送回复）。这种情况通常发生在两个进程都有 Binder 实体并互相对发请求时。假如驱动在进程 P2 发现该线程（T2），会要求 T2 处理 T1 的请求。因为 T2 既向 T1 发送请求未得到返回包，说明 T2 会阻塞在读取返回包的状态，可让 T2 分担部分工作，减少线程池使用率，比阻塞好

#### 6、数据包接收队列与（线程）等待队列管理

- 全局接收队列（to-do 队列）：在驱动中，每个进程有一个，存放不是发往特定线程的数据包，相应有一个全局等待队列，等待从全局接收队列里收数据的线程在该队列排队

- 线程私有 to-do 队列：存放发送给该线程的数据包，相应有等待队列，专用于本线程等待接收自己 to-do 队列里的数据，线程私有等待队列中最多只有一个线程（自己）

- 判断送入全局 to-do 队列还是特定线程 to-do 队列的规则：
  - Client 发给 Server 的请求数据包都提交到 Server 进程的全局 to-do 队列
    - 特例：上述 Binder 对工作线程启动的优化，优化后 T1 的请求不是提交给 P2 的全局 to-do 队列，而是送入了 T2 的私有 to-do 队列
  - 对同步请求的返回数据包（由 BC_REPLY 发送的包）都发送到发起请求的线程的私有 to-do 队列中
  - 一个线程不接收返回数据包就在全局等待队列中等待新任务，否则在其私有等待队列中等待 Server 的返回数据
  
- 同步请求交互过程中的线程一致性：
  - Client 端，等待返回包的线程必须是发送请求的线程
  - Server 端，发送对应返回数据包的线程必须是收到请求数据包的线程。因为返回数据包的目的 Binder 是驱动记录在收到请求数据包的线程里，如果发送返回包的线程不是收到请求包的线程，驱动无从知晓返回包送往何处
  
- 同步交互和异步交互的区别：同步交互的请求端在发出请求数据包后须等待应答端的返回数据包，而异步交互的发送端发出请求数据包后交互即结束

- Binder 同步、异步交互：一个 Binder 实体只要有一个异步交互没处理完，接下来发给该实体的异步交互包不再投递到 to-do 队列中，而是阻塞在驱动为该实体开辟的异步交互接收队列中（Binder 节点的 async_todo 域），这期间同步交互不受限制直接进入 to-do 队列获得处理，直到该异步交互处理完毕，下个异步交互方可脱离异步交互队列进入 to-do 队列中
  
  - 理由：因为同步交互的请求端需等待返回包，须迅速处理以免影响请求端响应速度，而异步交互稍微延时不会阻塞其它线程。所以用专门队列将过多的异步交互暂存，以免突发大量异步交互挤占 Server 端的处理能力或耗尽线程池里的线程，阻塞同步交互
  
  